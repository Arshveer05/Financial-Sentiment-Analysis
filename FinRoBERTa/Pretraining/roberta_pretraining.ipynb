{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets\n",
        "\n",
        "import os, time, glob\n",
        "# disable W&B & telemetry\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "MERGED_FILE = \"/content/merged_corpus.txt\"\n",
        "OUT_DIR = \"/content/roberta_pretrained_continued\"\n",
        "TMP_SHARD_DIR = \"/content/rt_shards\"     # temporary local shards\n",
        "COMBINED_DS_DIR = \"/content/rt_combined\" # saved combined dataset (optional)\n",
        "LOG_FILE = \"/content/rt_training_log.txt\"\n",
        "\n",
        "# tokenization / block params\n",
        "MAX_LEN = 512\n",
        "TOKENIZE_BATCH = 2000    # docs per tokenization batch (adjust based on memory)\n",
        "DOC_SEPARATOR = True     # insert EOS token between docs for boundary safety\n",
        "\n",
        "# training params (tuned for single T4)\n",
        "PER_DEVICE_BATCH = 4\n",
        "GRAD_ACC = 8\n",
        "EPOCHS = 3\n",
        "LR = 2e-5\n",
        "FP16 = True\n",
        "SAVE_STEPS = 2000\n",
        "LOGGING_STEPS = 200\n",
        "NUM_PROC = 2             # dataset.map workers where used\n",
        "\n",
        "os.makedirs(TMP_SHARD_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(COMBINED_DS_DIR, exist_ok=True)\n",
        "\n",
        "# ---------------- helper: find latest checkpoint ----------------\n",
        "def find_latest_checkpoint(path):\n",
        "    ckpts = sorted(glob.glob(os.path.join(path, \"checkpoint-*\")), key=os.path.getmtime)\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "resume_ckpt = find_latest_checkpoint(OUT_DIR)\n",
        "if resume_ckpt:\n",
        "    print(\"Auto-detected checkpoint to resume:\", resume_ckpt)\n",
        "else:\n",
        "    print(\"No checkpoint found in\", OUT_DIR, \"- will continue from roberta-base\")\n",
        "\n",
        "# ---------------- sanity check ----------------\n",
        "assert os.path.exists(MERGED_FILE), f\"merged corpus not found at {MERGED_FILE}\"\n",
        "\n",
        "# ---------------- Step 1: load docs ----------------\n",
        "print(\"Loading merged corpus...\")\n",
        "with open(MERGED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    docs = [d.strip() for d in f.read().split(\"\\n\\n\") if d.strip()]\n",
        "print(\"Total documents:\", len(docs))\n",
        "\n",
        "# ---------------- Step 2: tokenizer & tokenization (batched) ----------------\n",
        "from transformers import RobertaTokenizerFast\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "tokenizer.model_max_length = 10**12  # avoid >512 warnings while tokenizing before blockifying\n",
        "\n",
        "print(\"Tokenizing documents in batches...\")\n",
        "all_ids = []   # list of lists (per-doc token ids)\n",
        "for i in range(0, len(docs), TOKENIZE_BATCH):\n",
        "    batch = docs[i:i+TOKENIZE_BATCH]\n",
        "    enc = tokenizer(batch, add_special_tokens=False)\n",
        "    ids = enc[\"input_ids\"]\n",
        "    all_ids.extend(ids)\n",
        "    print(f\"Tokenized docs {i}..{i+len(batch)-1} -> total doc-tokens lists: {len(all_ids)}\")\n",
        "\n",
        "# optionally free docs list to save RAM\n",
        "# del docs\n",
        "\n",
        "# ---------------- Step 3: concatenate token ids and create contiguous 512-token blocks ----------------\n",
        "print(\"Concatenating token ids and creating 512-token blocks...\")\n",
        "flat = []\n",
        "EOS_ID = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id\n",
        "for seq in all_ids:\n",
        "    if DOC_SEPARATOR:\n",
        "        # add EOS between docs to avoid cross-doc bleed\n",
        "        flat.extend(seq + ([EOS_ID] if EOS_ID is not None else []))\n",
        "    else:\n",
        "        flat.extend(seq)\n",
        "\n",
        "total_tokens = len(flat)\n",
        "num_blocks = total_tokens // MAX_LEN\n",
        "total_used = num_blocks * MAX_LEN\n",
        "print(\"Total tokens (concatenated):\", total_tokens)\n",
        "print(\"Number of full 512-token blocks:\", num_blocks)\n",
        "if num_blocks == 0:\n",
        "    raise RuntimeError(\"Not enough tokens to form a single 512-token block. Check your corpus.\")\n",
        "\n",
        "# create blocks (list of lists)\n",
        "blocks = [flat[i:i+MAX_LEN] for i in range(0, total_used, MAX_LEN)]\n",
        "print(\"Created blocks length:\", len(blocks))\n",
        "\n",
        "# free memory for 'flat' and 'all_ids' if needed\n",
        "del flat\n",
        "del all_ids\n",
        "\n",
        "# ---------------- Step 4: convert blocks into HF Dataset ----------------\n",
        "from datasets import Dataset\n",
        "print(\"Building Hugging Face Dataset from blocks (this may take a moment)...\")\n",
        "ds = Dataset.from_dict({\"input_ids\": blocks})\n",
        "# create attention masks (all ones since blocks are full-length)\n",
        "import torch\n",
        "attn = [[1]*MAX_LEN for _ in range(len(blocks))]\n",
        "ds = ds.add_column(\"attention_mask\", attn)\n",
        "print(\"Dataset created. Examples:\", len(ds))\n",
        "# save for reuse\n",
        "try:\n",
        "    ds.save_to_disk(COMBINED_DS_DIR)\n",
        "    print(\"Saved combined dataset to:\", COMBINED_DS_DIR)\n",
        "except Exception as e:\n",
        "    print(\"Warning: could not save combined dataset:\", e)\n",
        "\n",
        "# set format to torch\n",
        "ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# ---------------- Step 5: prepare model & collator (continued pretraining) ----------------\n",
        "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling\n",
        "print(\"Loading roberta-base model for continued pretraining...\")\n",
        "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "# ---------------- Step 6: TrainingArguments & Trainer ----------------\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    overwrite_output_dir=False,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    learning_rate=LR,\n",
        "    fp16=FP16,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=[],   # disable tracking/reporting\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=ds,\n",
        ")\n",
        "\n",
        "# ---------------- Step 7: Train (resume if checkpoint exists) ----------------\n",
        "print(\"Starting training. Resume checkpoint:\", resume_ckpt)\n",
        "start_time = time.time()\n",
        "try:\n",
        "    if resume_ckpt:\n",
        "        trainer.train(resume_from_checkpoint=resume_ckpt)\n",
        "    else:\n",
        "        trainer.train()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Interrupted by user.\")\n",
        "except Exception as e:\n",
        "    print(\"Training crashed:\", repr(e))\n",
        "    raise\n",
        "finally:\n",
        "    # save final model and tokenizer\n",
        "    trainer.save_model(OUT_DIR)\n",
        "    tokenizer.save_pretrained(OUT_DIR)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Training finished (or stopped). Time elapsed: {elapsed/60:.2f} minutes\")\n",
        "    print(\"Model + tokenizer saved to:\", OUT_DIR)\n",
        "    print(\"Logs saved to:\", LOG_FILE)"
      ],
      "metadata": {
        "id": "R_81x1jFauUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}