{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers scikit-learn pandas nltk tqdm torch torchvision tabulate --quiet\n",
        "\n",
        "import torch, pandas as pd, numpy as np, nltk\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from tabulate import tabulate\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# 1. PREPROCESSING \n",
        "# ============================================================\n",
        "\n",
        "DATA_FILE = \"/content/polygon_news_1.csv\"\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "\n",
        "required_cols = [\"title\", \"description\", \"sentiment\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Column '{col}' missing. Available: {df.columns.tolist()}\")\n",
        "\n",
        "df[\"text\"] = df[\"title\"].astype(str) + \" \" + df[\"description\"].astype(str)\n",
        "df = df[[\"text\", \"sentiment\"]].dropna()\n",
        "df = df.rename(columns={\"sentiment\": \"label\"})\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
        "LABELS = list(le.classes_)\n",
        "print(\"Initial Classes:\", LABELS)\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# ============================================================\n",
        "# 2. REMOVE RARE CLASSES (<2 samples) â€” required for stratify\n",
        "# ============================================================\n",
        "\n",
        "counts = df[\"label_id\"].value_counts()\n",
        "valid_ids = counts[counts >= 2].index.tolist()\n",
        "df = df[df[\"label_id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "# Refit encoder\n",
        "le = LabelEncoder()\n",
        "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
        "LABELS = list(le.classes_)\n",
        "\n",
        "print(\"\\nAfter removing rare classes:\")\n",
        "print(\"Classes:\", LABELS)\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# ============================================================\n",
        "# 3. SAFE 80/10/10 SPLIT\n",
        "# ============================================================\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.20, stratify=df[\"label_id\"], random_state=42\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, stratify=temp_df[\"label_id\"], random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. LOAD FINBERT\n",
        "# ============================================================\n",
        "\n",
        "FINBERT_MODEL = \"ProsusAI/finbert\"\n",
        "print(\"\\nðŸ”¹ Loading FinBERT...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n",
        "finbert = AutoModel.from_pretrained(FINBERT_MODEL).to(DEVICE)\n",
        "finbert.eval()\n",
        "\n",
        "EMBED_DIM = finbert.config.hidden_size\n",
        "print(\"FinBERT Embedding Dim:\", EMBED_DIM)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Dataset + Collate\n",
        "# ============================================================\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label_id\"].tolist()\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx): return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    attn_mask = enc[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = finbert(input_ids, attention_mask=attn_mask)\n",
        "        emb = outputs.last_hidden_state.mean(dim=1)   # (batch, 768)\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
        "    return emb, labels\n",
        "\n",
        "train_loader = DataLoader(TextDataset(train_df), batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TextDataset(val_df),   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(TextDataset(test_df),  batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ============================================================\n",
        "# 6. SIMPLE LINEAR CLASSIFIER (NO LSTM)\n",
        "# ============================================================\n",
        "\n",
        "class FinBertClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = FinBertClassifier(EMBED_DIM, len(LABELS)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ============================================================\n",
        "# 7. EVALUATION FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss, all_y, all_pred = 0, [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            out = model(X)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            all_y.extend(y.cpu().numpy())\n",
        "            all_pred.extend(pred.cpu().numpy())\n",
        "\n",
        "    return (\n",
        "        total_loss / len(loader),\n",
        "        accuracy_score(all_y, all_pred),\n",
        "        f1_score(all_y, all_pred, average=\"macro\"),\n",
        "        all_y,\n",
        "        all_pred\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 8. TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "EPOCHS = 6\n",
        "for ep in range(EPOCHS):\n",
        "    model.train()\n",
        "    running = 0\n",
        "\n",
        "    for X, y in tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\"):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item()\n",
        "\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(val_loader)\n",
        "\n",
        "    print(f\"\\nEpoch {ep+1}: \"\n",
        "          f\"TrainLoss={running/len(train_loader):.4f}  \"\n",
        "          f\"ValLoss={val_loss:.4f}  ValAcc={val_acc:.4f}  ValF1={val_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. FINAL TEST RESULTS\n",
        "# ============================================================\n",
        "\n",
        "test_loss, test_acc, test_f1, y_true, y_pred = evaluate(test_loader)\n",
        "\n",
        "print(\"\\n--- FINAL TEST RESULTS ---\")\n",
        "print(f\"Loss={test_loss:.4f}  Accuracy={test_acc:.4f}  F1={test_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. SAFE CLASSIFICATION REPORT\n",
        "# ============================================================\n",
        "\n",
        "valid_labels = unique_labels(y_true, y_pred)\n",
        "valid_label_names = [LABELS[i] for i in valid_labels]\n",
        "\n",
        "print(\"\\nPredicted Classes:\", valid_label_names)\n",
        "\n",
        "report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=valid_labels,\n",
        "    target_names=valid_label_names,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "df_report = df_report[[\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
        "df_report[[\"precision\", \"recall\", \"f1-score\"]] = df_report[[\"precision\", \"recall\", \"f1-score\"]].round(4)\n",
        "df_report[\"support\"] = df_report[\"support\"].astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(tabulate(df_report, headers=\"keys\", tablefmt=\"github\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6c3900291bfa43eb8a1033bceb5b32d7",
            "09273d40cc1044578d200a1b077407d4",
            "862d6a7813794cdabbc30790414c8da1",
            "91ec682bcc86483ba9b75a9e649ece37",
            "df8ec10abff14af6b945015fafbba515",
            "832b700cf4f34812a231a574c24198de",
            "cf5cdc764c2541b7afbd350220134d10",
            "f668bac2de1447eab5ae4900042aab0b",
            "bb561d63eb61436e848ac9faf73de311",
            "063da157b13e476c9e5c8518fe578f82",
            "1f1e76ee13f04d36b76420caebf1625c",
            "fc8615a084d7458e8c1eeade4fceab8e",
            "7699e7a8e9b94ccebd6a1377a0d46d73",
            "db763c231eed478398e7f5bad0237c5b",
            "95635dc1df014e9e8ec43be3fe492a27",
            "e410b7aedae9439188711a1d8148abb0",
            "29952c29b41f4227bdbd2680a122a3ab",
            "86947d50121b466caf60d9933691d61d",
            "4bc91d299d4a49ea9838d3142d9054d7",
            "0a031192e74c44e08404216bccaaad64",
            "ea8d1ebe18554fe1999a386030cfcaf8",
            "c0227b4fe4444907bbea1bb1dffddd05",
            "6c03e6bf78fe4906a48acf6007a22b1d",
            "315cd1611f6a48aaa46bf5f0ef623b34",
            "99660bbfd0354628b1c662bba8d0d9f6",
            "667d2b7c46ea456a87ee729e35ef8717",
            "c9f00ecbe6484c8e9fff7512b63cf56d",
            "95432997a3b943cba03c3938ec586e91",
            "1a5c32bd84274913b5b945298ece99c0",
            "eae8d3cd7d454b37ac41263a7c5c12a9",
            "1d29f8c1baa0433588ff622c3aab7f69",
            "6fc6f3aa62e34f87ae10f75c5da3b8e5",
            "e8f30834707446f1a9ba6e90b01f3b98",
            "65a6cc6a167f417db45cf73fb3e6509d",
            "039531363ef54504b74705d7e4bfea92",
            "1acdba064e3942ea990b0c01013221bd",
            "8e02d750a29c477a9f3595f53da90a89",
            "7f8bb31ce25044a58902ff33ec666dc7",
            "4abb70504a0e468e8be8db728b3ee00b",
            "a61e13713e0f46ecb5abdaf79e0681fb",
            "22819de4faa3434783a35eb562fc96cd",
            "10ad3536d3f743ff9efc4a14c7baf454",
            "5fcca1ecd0e4417985aceb49ab683f19",
            "044e3b119edf4fdb9dcaaf113c773b98",
            "e2b7cd7b591945ea9cab152a55d6d30f",
            "fc954ac076174d20b6dbad9cee3f0b81",
            "add0c8c5babf4bccbbe6d09beb8b3f46",
            "9554fb2d4e78461c88621b7a020fa079",
            "8adaeec84b9e4839b9bc84eb2a7c2360",
            "ae2d3616eb5a441a8126985d4b92a006",
            "3ed91815c6c8497180f1bd0f7c4bb267",
            "2f82b4c8ff4a4f94aff11300e36d85ab",
            "b9aa7cb02ecd4eddb645a4704871e4c2",
            "0aebba47a5f0497d914f755d24def97a",
            "ad863e97d5394738acac96b6d0d8f1b8",
            "57e45de51b3d4a92b28695b1ad3f8226",
            "2242e8f87aa5431da8aacddf6ba46209",
            "c48a67f27dae49248c78e1a984b97ebe",
            "9ec64e471fe34b34b562fe915a03d93a",
            "a79ebef2359e405bb3657866ba68fe8d",
            "4dfd435328c54030a14b1216450b1baa",
            "c998e0044a214b629b60e4d727c73547",
            "a1fa3ceb9b3e4a729e0b151ae738ab13",
            "526070d008b74ea78c07fcfce407e47f",
            "49afa3e168ef4c2b90600b00d50475cc",
            "32af150b95f54aa782efbb7f74e5485f"
          ]
        },
        "id": "-UzTOeY-KnAi",
        "outputId": "584e5b23-add3-45b6-995f-2bbfe13b23a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initial Classes: ['mixed', 'negative', 'neutral', 'neutral/positive', 'positive']\n",
            "label\n",
            "positive            3626\n",
            "neutral             1270\n",
            "negative             649\n",
            "mixed                  2\n",
            "neutral/positive       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After removing rare classes:\n",
            "Classes: ['mixed', 'negative', 'neutral', 'positive']\n",
            "label\n",
            "positive    3626\n",
            "neutral     1270\n",
            "negative     649\n",
            "mixed          2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train=4437, Val=555, Test=555\n",
            "\n",
            "ðŸ”¹ Loading FinBERT...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c3900291bfa43eb8a1033bceb5b32d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc8615a084d7458e8c1eeade4fceab8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c03e6bf78fe4906a48acf6007a22b1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65a6cc6a167f417db45cf73fb3e6509d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2b7cd7b591945ea9cab152a55d6d30f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FinBERT Embedding Dim: 768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/6:   1%|          | 2/278 [00:00<00:39,  6.99it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57e45de51b3d4a92b28695b1ad3f8226"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:29<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: TrainLoss=0.5571  ValLoss=0.5196  ValAcc=0.7928  ValF1=0.6998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:26<00:00, 10.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: TrainLoss=0.5087  ValLoss=0.5031  ValAcc=0.7928  ValF1=0.6792\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:26<00:00, 10.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: TrainLoss=0.4825  ValLoss=0.5207  ValAcc=0.7694  ValF1=0.6378\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:27<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: TrainLoss=0.4784  ValLoss=0.4744  ValAcc=0.7874  ValF1=0.6988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:26<00:00, 10.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: TrainLoss=0.4712  ValLoss=0.4808  ValAcc=0.7964  ValF1=0.6814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:26<00:00, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: TrainLoss=0.4608  ValLoss=0.4823  ValAcc=0.7946  ValF1=0.6769\n",
            "\n",
            "--- FINAL TEST RESULTS ---\n",
            "Loss=0.4755  Accuracy=0.7928  F1=0.7005\n",
            "\n",
            "Predicted Classes: ['negative', 'neutral', 'positive']\n",
            "\n",
            "Classification Report:\n",
            "|              |   precision |   recall |   f1-score |   support |\n",
            "|--------------|-------------|----------|------------|-----------|\n",
            "| negative     |      0.7018 |   0.6154 |     0.6557 |        65 |\n",
            "| neutral      |      0.6337 |   0.5039 |     0.5614 |       127 |\n",
            "| positive     |      0.8463 |   0.9256 |     0.8842 |       363 |\n",
            "| accuracy     |      0.7928 |   0.7928 |     0.7928 |         0 |\n",
            "| macro avg    |      0.7273 |   0.6816 |     0.7005 |       555 |\n",
            "| weighted avg |      0.7807 |   0.7928 |     0.7836 |       555 |\n"
          ]
        }
      ]
    }
  ]
}
