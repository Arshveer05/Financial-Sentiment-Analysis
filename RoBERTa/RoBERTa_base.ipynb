{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7Drky1dRMIS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIUqX_QtMIj-",
        "outputId": "415534ba-ac13-458d-edbc-ef12eb6407bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Initial Classes: ['mixed', 'negative', 'neutral', 'neutral/positive', 'positive']\n",
            "label\n",
            "positive            3626\n",
            "neutral             1270\n",
            "negative             649\n",
            "mixed                  2\n",
            "neutral/positive       1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After removing rare classes:\n",
            "Classes: ['mixed', 'negative', 'neutral', 'positive']\n",
            "label\n",
            "positive    3626\n",
            "neutral     1270\n",
            "negative     649\n",
            "mixed          2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train=4437, Val=555, Test=555\n",
            "\n",
            "ðŸ”¹ Loading RoBERTa...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa Embedding Dim: 768\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:28<00:00,  9.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1: TrainLoss=0.7293  ValLoss=0.5902  ValAcc=0.7604  ValF1=0.6083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:25<00:00, 10.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2: TrainLoss=0.5898  ValLoss=0.5168  ValAcc=0.7820  ValF1=0.6543\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:25<00:00, 11.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3: TrainLoss=0.5429  ValLoss=0.4731  ValAcc=0.8036  ValF1=0.7118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:25<00:00, 10.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4: TrainLoss=0.5431  ValLoss=0.4707  ValAcc=0.8000  ValF1=0.7000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:25<00:00, 10.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5: TrainLoss=0.5194  ValLoss=0.4921  ValAcc=0.7856  ValF1=0.6415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 278/278 [00:25<00:00, 11.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6: TrainLoss=0.5000  ValLoss=0.4710  ValAcc=0.7892  ValF1=0.6697\n",
            "\n",
            "--- FINAL TEST RESULTS ---\n",
            "Loss=0.4610  Accuracy=0.7946  F1=0.6913\n",
            "\n",
            "Predicted Classes: ['negative', 'neutral', 'positive']\n",
            "\n",
            "Classification Report:\n",
            "|              |   precision |   recall |   f1-score |   support |\n",
            "|--------------|-------------|----------|------------|-----------|\n",
            "| negative     |      0.6579 |   0.7692 |     0.7092 |        65 |\n",
            "| neutral      |      0.6866 |   0.3622 |     0.4742 |       127 |\n",
            "| positive     |      0.8374 |   0.9504 |     0.8903 |       363 |\n",
            "| accuracy     |      0.7946 |   0.7946 |     0.7946 |         0 |\n",
            "| macro avg    |      0.7273 |   0.6939 |     0.6913 |       555 |\n",
            "| weighted avg |      0.7818 |   0.7946 |     0.7739 |       555 |\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers scikit-learn pandas nltk tqdm torch torchvision tabulate --quiet\n",
        "\n",
        "import torch, pandas as pd, numpy as np, nltk\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from tabulate import tabulate\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# 1. PREPROCESSING (title + description ONLY)\n",
        "# ============================================================\n",
        "\n",
        "DATA_FILE = \"/content/polygon_news_1.csv\"\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "\n",
        "required_cols = [\"title\", \"description\",\"keywords\", \"sentiment\"]\n",
        "for col in required_cols:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Column '{col}' missing. Available: {df.columns.tolist()}\")\n",
        "\n",
        "df[\"text\"] = df[\"title\"].astype(str) + \" \" + df[\"description\"].astype(str)\n",
        "df = df[[\"text\", \"sentiment\"]].dropna()\n",
        "df = df.rename(columns={\"sentiment\": \"label\"})\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
        "LABELS = list(le.classes_)\n",
        "print(\"Initial Classes:\", LABELS)\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# ============================================================\n",
        "# 2. REMOVE RARE CLASSES (<2 samples)\n",
        "# ============================================================\n",
        "\n",
        "counts = df[\"label_id\"].value_counts()\n",
        "valid_ids = counts[counts >= 2].index.tolist()\n",
        "df = df[df[\"label_id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "# Refit encoder after removing rare classes\n",
        "le = LabelEncoder()\n",
        "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
        "LABELS = list(le.classes_)\n",
        "\n",
        "print(\"\\nAfter removing rare classes:\")\n",
        "print(\"Classes:\", LABELS)\n",
        "print(df[\"label\"].value_counts())\n",
        "\n",
        "# ============================================================\n",
        "# 3. SAFE 80/10/10 SPLIT\n",
        "# ============================================================\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.20, stratify=df[\"label_id\"], random_state=42\n",
        ")\n",
        "\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.50, stratify=temp_df[\"label_id\"], random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. LOAD ROBERTA\n",
        "# ============================================================\n",
        "\n",
        "ROBERTA_MODEL = \"roberta-base\"\n",
        "\n",
        "print(\"\\nðŸ”¹ Loading RoBERTa...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL)\n",
        "roberta = AutoModel.from_pretrained(ROBERTA_MODEL).to(DEVICE)\n",
        "roberta.eval()\n",
        "\n",
        "EMBED_DIM = roberta.config.hidden_size  # usually 768\n",
        "print(\"RoBERTa Embedding Dim:\", EMBED_DIM)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Dataset + Collate\n",
        "# ============================================================\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df[\"text\"].tolist()\n",
        "        self.labels = df[\"label_id\"].tolist()\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx): return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"].to(DEVICE)\n",
        "    attn_mask = enc[\"attention_mask\"].to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = roberta(input_ids, attention_mask=attn_mask)\n",
        "        emb = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
        "    return emb, labels\n",
        "\n",
        "train_loader = DataLoader(TextDataset(train_df), batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(TextDataset(val_df),   batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(TextDataset(test_df),  batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ============================================================\n",
        "# 6. SIMPLE LINEAR CLASSIFIER (NO LSTM)\n",
        "# ============================================================\n",
        "\n",
        "class RobertaClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = RobertaClassifier(EMBED_DIM, len(LABELS)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ============================================================\n",
        "# 7. EVALUATION FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss, all_y, all_pred = 0, [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            out = model(X)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            all_y.extend(y.cpu().numpy())\n",
        "            all_pred.extend(pred.cpu().numpy())\n",
        "\n",
        "    return (\n",
        "        total_loss / len(loader),\n",
        "        accuracy_score(all_y, all_pred),\n",
        "        f1_score(all_y, all_pred, average=\"macro\"),\n",
        "        all_y,\n",
        "        all_pred\n",
        "    )\n",
        "\n",
        "# ============================================================\n",
        "# 8. TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "EPOCHS = 6\n",
        "for ep in range(EPOCHS):\n",
        "    model.train()\n",
        "    running = 0\n",
        "\n",
        "    for X, y in tqdm(train_loader, desc=f\"Epoch {ep+1}/{EPOCHS}\"):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running += loss.item()\n",
        "\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(val_loader)\n",
        "\n",
        "    print(f\"\\nEpoch {ep+1}: \"\n",
        "          f\"TrainLoss={running/len(train_loader):.4f}  \"\n",
        "          f\"ValLoss={val_loss:.4f}  ValAcc={val_acc:.4f}  ValF1={val_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. FINAL TEST RESULTS\n",
        "# ============================================================\n",
        "\n",
        "test_loss, test_acc, test_f1, y_true, y_pred = evaluate(test_loader)\n",
        "\n",
        "print(\"\\n--- FINAL TEST RESULTS ---\")\n",
        "print(f\"Loss={test_loss:.4f}  Accuracy={test_acc:.4f}  F1={test_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. SAFE CLASSIFICATION REPORT\n",
        "# ============================================================\n",
        "\n",
        "valid_labels = unique_labels(y_true, y_pred)\n",
        "valid_label_names = [LABELS[i] for i in valid_labels]\n",
        "\n",
        "print(\"\\nPredicted Classes:\", valid_label_names)\n",
        "\n",
        "report = classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=valid_labels,\n",
        "    target_names=valid_label_names,\n",
        "    output_dict=True\n",
        ")\n",
        "\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "df_report = df_report[[\"precision\", \"recall\", \"f1-score\", \"support\"]]\n",
        "df_report[[\"precision\", \"recall\", \"f1-score\"]] = df_report[[\"precision\", \"recall\", \"f1-score\"]].round(4)\n",
        "df_report[\"support\"] = df_report[\"support\"].astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(tabulate(df_report, headers=\"keys\", tablefmt=\"github\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
