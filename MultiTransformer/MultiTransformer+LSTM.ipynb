{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7Drky1dRMIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Multi-Transformer Embeddings + LSTM (PyTorch 2.x)\n",
        "# ============================================================\n",
        "\n",
        "!pip install transformers scikit-learn pandas nltk tqdm torch torchvision tabulate --quiet\n",
        "\n",
        "import torch, numpy as np, pandas as pd, nltk\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tabulate import tabulate\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"âœ… Using device:\", DEVICE)\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load and clean dataset\n",
        "# ============================================================\n",
        "\n",
        "DATA_FILE = \"/content/polygon_news_1.csv\"  # <-- Update this path if needed\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "\n",
        "# Combine all text columns except sentiment\n",
        "text_columns = [col for col in df.columns if col != 'sentiment']\n",
        "df['text'] = df[text_columns].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
        "\n",
        "# Keep only text and sentiment columns\n",
        "df = df[['text', 'sentiment']].dropna()\n",
        "df = df.rename(columns={'sentiment': 'label'})\n",
        "\n",
        "# === Map 5 â†’ 3 sentiment categories ===\n",
        "mapping = {\n",
        "    'mixed': 'neutral',\n",
        "    'neutral': 'neutral',\n",
        "    'neutral/positive': 'positive',\n",
        "    'positive': 'positive',\n",
        "    'negative': 'negative'\n",
        "}\n",
        "df['label'] = df['label'].map(mapping).fillna('neutral')\n",
        "\n",
        "# Encode numeric labels\n",
        "le = LabelEncoder()\n",
        "df['label_id'] = le.fit_transform(df['label'])\n",
        "LABELS = list(le.classes_)\n",
        "\n",
        "print(\"âœ… Label mapping used:\", mapping)\n",
        "print(\"Classes:\", LABELS)\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Remove rare labels\n",
        "label_counts = df['label_id'].value_counts()\n",
        "df = df[df['label_id'].isin(label_counts[label_counts > 1].index)]\n",
        "\n",
        "# Train/Val/Test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label_id'])\n",
        "val_df, test_df  = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df['label_id'])\n",
        "\n",
        "print(f\"Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Load MULTIPLE Transformer Models\n",
        "# ============================================================\n",
        "\n",
        "model_names = {\n",
        "    \"bert\": \"bert-base-uncased\",\n",
        "    \"roberta\": \"roberta-base\",\n",
        "    \"distilbert\": \"distilbert-base-uncased\"\n",
        "}\n",
        "\n",
        "models = {}\n",
        "for key, name in model_names.items():\n",
        "    print(f\"ðŸ”¹ Loading {name} ...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name).to(DEVICE)\n",
        "    model.eval()\n",
        "    models[key] = (tokenizer, model)\n",
        "\n",
        "# Calculate total embedding dimension\n",
        "total_embed_dim = sum(m[1].config.hidden_size for m in models.values())\n",
        "print(\"âœ… Total concatenated embedding size:\", total_embed_dim)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Dataset + Multi-Transformer Collate Function\n",
        "# ============================================================\n",
        "\n",
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.texts = df['text'].astype(str).tolist()\n",
        "        self.labels = df['label_id'].tolist()\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_batch_multi(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    all_embeds = []\n",
        "\n",
        "    for key, (tokenizer, model) in models.items():\n",
        "        enc = tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        input_ids = enc['input_ids'].to(DEVICE)\n",
        "        attn_mask = enc['attention_mask'].to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids, attention_mask=attn_mask)\n",
        "            emb = out.last_hidden_state.mean(dim=1)  # mean-pool across tokens\n",
        "        all_embeds.append(emb)\n",
        "\n",
        "    # Concatenate embeddings from all transformer models\n",
        "    combined_emb = torch.cat(all_embeds, dim=1)\n",
        "    labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)\n",
        "    return combined_emb, labels\n",
        "\n",
        "# DataLoaders\n",
        "train_ds, val_ds, test_ds = TransformerDataset(train_df), TransformerDataset(val_df), TransformerDataset(test_df)\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_batch_multi)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_batch_multi)\n",
        "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False, collate_fn=collate_batch_multi)\n",
        "\n",
        "# ============================================================\n",
        "# 4. LSTM Model Definition\n",
        "# ============================================================\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=bidirectional,\n",
        "                            dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * (2 if bidirectional else 1), 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, seq_len=1, input_dim)\n",
        "        output, _ = self.lstm(x)\n",
        "        pooled = output[:, -1, :]\n",
        "        return self.fc(pooled)\n",
        "\n",
        "model = LSTMClassifier(total_embed_dim, hidden_dim=256, output_dim=len(LABELS)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Training & Evaluation\n",
        "# ============================================================\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    all_labels, all_preds = [], []\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(out, dim=1)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    avg_loss = total_loss / max(1, len(loader))\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, acc, f1, all_labels, all_preds\n",
        "\n",
        "# --- Training Loop ---\n",
        "EPOCHS = 8\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    val_loss, val_acc, val_f1, _, _ = evaluate(val_loader)\n",
        "    print(f\"\\nEpoch {epoch+1}: TrainLoss={running_loss/len(train_loader):.4f} \"\n",
        "          f\"ValLoss={val_loss:.4f}  ValAcc={val_acc:.4f}  ValF1={val_f1:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Final Test Results\n",
        "# ============================================================\n",
        "\n",
        "test_loss, test_acc, test_f1, y_true, y_pred = evaluate(test_loader)\n",
        "print(\"\\n--- ðŸ§¾ Final Test Results ---\")\n",
        "print(f\"Loss={test_loss:.4f}  Accuracy={test_acc:.4f}  F1={test_f1:.4f}\")\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=LABELS, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "df_report = df_report[['precision', 'recall', 'f1-score', 'support']]\n",
        "df_report[['precision', 'recall', 'f1-score']] = df_report[['precision', 'recall', 'f1-score']].round(4)\n",
        "df_report['support'] = df_report['support'].astype(int)\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(tabulate(df_report, headers='keys', tablefmt='github'))\n"
      ],
      "metadata": {
        "id": "FCh1yDl6LbKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb266662-36b0-453b-da76-051d2afdd76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Using device: cuda\n",
            "âœ… Label mapping used: {'mixed': 'neutral', 'neutral': 'neutral', 'neutral/positive': 'positive', 'positive': 'positive', 'negative': 'negative'}\n",
            "Classes: ['negative', 'neutral', 'positive']\n",
            "label\n",
            "positive    3627\n",
            "neutral     1272\n",
            "negative     649\n",
            "Name: count, dtype: int64\n",
            "Train=4715, Val=416, Test=417\n",
            "ðŸ”¹ Loading bert-base-uncased ...\n",
            "ðŸ”¹ Loading roberta-base ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Loading distilbert-base-uncased ...\n",
            "âœ… Total concatenated embedding size: 2304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:21<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: TrainLoss=0.6092 ValLoss=0.4267  ValAcc=0.8197  ValF1=0.7274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: TrainLoss=0.4361 ValLoss=0.4739  ValAcc=0.7861  ValF1=0.7361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3: TrainLoss=0.3795 ValLoss=0.3722  ValAcc=0.8173  ValF1=0.7337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: TrainLoss=0.3491 ValLoss=0.3124  ValAcc=0.8582  ValF1=0.7865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5: TrainLoss=0.3139 ValLoss=0.2977  ValAcc=0.8750  ValF1=0.8179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6: TrainLoss=0.2908 ValLoss=0.3373  ValAcc=0.8534  ValF1=0.7683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7: TrainLoss=0.2776 ValLoss=0.2871  ValAcc=0.8846  ValF1=0.8131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295/295 [01:20<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8: TrainLoss=0.2765 ValLoss=0.2622  ValAcc=0.8846  ValF1=0.8325\n",
            "\n",
            "--- ðŸ§¾ Final Test Results ---\n",
            "Loss=0.2009  Accuracy=0.9305  F1=0.8832\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "|              |   precision |   recall |   f1-score |   support |\n",
            "|--------------|-------------|----------|------------|-----------|\n",
            "| negative     |      0.8974 |   0.7292 |     0.8046 |        48 |\n",
            "| neutral      |      0.8241 |   0.9271 |     0.8725 |        96 |\n",
            "| positive     |      0.9778 |   0.967  |     0.9724 |       273 |\n",
            "| accuracy     |      0.9305 |   0.9305 |     0.9305 |         0 |\n",
            "| macro avg    |      0.8998 |   0.8744 |     0.8832 |       417 |\n",
            "| weighted avg |      0.9331 |   0.9305 |     0.9301 |       417 |\n"
          ]
        }
      ]
    }
  ]
}